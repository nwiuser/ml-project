import numpy as np
import matplotlib.pyplot as plt
import streamlit as st
import time
import random
from qlearn import QLearningAgent, actions
from maze import Maze, maze_fire, maze_medium, maze_piece_cake


# <------ les param√©tres -------->
    
goal_reward = 100
wall_penalty = -10
step_penalty = 10

# maze_piece_cake.goal_position = (9,9)
# maze_medium.goal_position = (20,20)
# maze_fire.goal_position = (43,35)

maze = Maze(maze_fire, (0,0), (43,35))
agent = QLearningAgent(maze=maze)

# <------- les fonctions ------->

def finish_episode(agent, maze, current_episode, train=True):
    current_state = maze.start_position
    done = False
    episode_reward = 0
    episode_step = 0
    path = [current_state]

    while not done:
        # Choisir l'action bas√©e sur l'√©tat actuel
        action = agent.choose_action(current_state, current_episode)
        state_1 = current_state[0] + actions[action][0]
        state_2 = current_state[1] + actions[action][1]
        next_state = (state_1, state_2)

        # V√©rification des limites et des murs
        if (
            next_state[0] < 0 or next_state[0] >= maze.width or
            next_state[1] < 0 or next_state[1] >= maze.height
        ):
            # P√©nalit√© pour une tentative hors limites
            reward = wall_penalty
            next_state = current_state

        elif maze.maze[next_state[1]][next_state[0]] == 1:
            # P√©nalit√© pour heurter un mur
            reward = wall_penalty
            next_state = current_state

        elif next_state == maze.goal_position:
            # R√©compense pour atteindre l'objectif
            path.append(next_state)
            reward = goal_reward
            done = True

        else:
            # Mouvement valide
            path.append(next_state)
            reward = step_penalty

        # Mise √† jour des r√©compenses et des √©tapes
        episode_reward += reward
        episode_step += 1

        # Entra√Ænement si activ√©
        if train:
            agent.update_q_table(current_state, action, reward, next_state)

        # Mise √† jour de l'√©tat courant
        current_state = next_state

    return episode_reward, episode_step, path



def test_agent(agent, maze, num_episodes=1):
  episode_reward, episode_step, path = finish_episode(agent, maze, 1, train=False)

  print("Learned Path:")
  for row, col in path:
    print(f"({row}, {col})-> ", end='')
  print('Goal!')

  print("Number of steps : ", episode_step)
  print("Total reward : ", episode_reward)

  # Clear the existing plot if any
  if plt.gcf().get_axes():
    plt.cla()

  # Visuualize the maze using matplotlib
  plt.figure(figsize=(10,10))
  plt.imshow(maze.maze, cmap='gray')

  if np.array_equal(maze.maze, maze_piece_cake):
      size = (5, 5)
      fontsize = 20
  elif np.array_equal(maze.maze, maze_medium):
      size = (10, 10)
      fontsize = 15
  elif np.array_equal(maze.maze, maze_fire):
      size = (15, 15)
      fontsize = 10
  else:
      size = (5, 5)
      fontsize = 15

  # Mark the start position (red, 'S') and goal position (green 'G') in the maze
  plt.text(maze.start_position[0], maze.start_position[1], 'S', ha='center', va='center', color='red', fontsize=fontsize)
  plt.text(maze.goal_position[0], maze.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=fontsize)

  # Mark the agent's path with blue '#' symbols
  for position in path:
    plt.text(position[0], position[1], '#', va='center', color='yellow', fontsize=fontsize)

  # Remove axis ticks and grid lines for a cleaner visualization
  plt.xticks([]), plt.yticks([])
  plt.grid(color='black', linewidth=2)
  plt.show()

  return episode_reward, episode_step, path



def train_agent(agent, maze, num_episodes=100):
    st.title("Entra√Ænement de l'Agent - Visualisation en Temps R√©el")
    
    # Variables pour stocker les r√©compenses et les √©tapes par √©pisode
    episode_rewards = []
    episode_steps = []

    # Placeholders Streamlit pour mettre √† jour les graphiques et statistiques
    reward_chart = st.empty()
    steps_chart = st.empty()
    stats = st.empty()
    progress_bar = st.progress(0)

    # D√©marrer le chronom√®tre
    start_time = time.time()

    for episode in range(num_episodes):
        # Terminer un √©pisode et collecter les r√©sultats
        episode_reward, episode_step, path = finish_episode(agent, maze, episode, train=True)

        episode_rewards.append(episode_reward)
        episode_steps.append(episode_step)

        # Afficher les statistiques moyennes apr√®s chaque √©pisode
        average_reward = sum(episode_rewards) / len(episode_rewards)
        average_steps = sum(episode_steps) / len(episode_steps)

        elapsed_time = time.time() - start_time  # Temps √©coul√© en secondes
        formatted_time = time.strftime("%H:%M:%S", time.gmtime(elapsed_time))  # Format en heures:minutes:secondes

        stats.markdown(f"""
        **√âpisode : {episode + 1}/{num_episodes}**
        - R√©compense cumul√©e : {episode_reward}
        - Pas pris : {episode_step}
        - R√©compense moyenne : {average_reward:.2f}
        - Nombre moyen de pas : {average_steps:.2f}
        - Temps √©coul√© : {formatted_time}
        """)

        # Graphique des r√©compenses
        with reward_chart.container():
            fig, ax = plt.subplots()
            ax.plot(episode_rewards, label="R√©compense par √©pisode", color="blue")
            ax.set_title("R√©compenses cumul√©es par √©pisode")
            ax.set_xlabel("√âpisode")
            ax.set_ylabel("R√©compense cumul√©e")
            ax.legend()
            st.pyplot(fig)

        # Graphique des √©tapes par √©pisode
        with steps_chart.container():
            fig, ax = plt.subplots()
            ax.plot(episode_steps, label="√âtapes par √©pisode", color="green")
            ax.set_title("Nombre d'√©tapes par √©pisode")
            ax.set_xlabel("√âpisode")
            ax.set_ylabel("√âtapes prises")
            ax.legend()
            st.pyplot(fig)

        # Mettre √† jour la barre de progression
        progress_bar.progress((episode + 1) / num_episodes)

    # Calcul du temps total d'entra√Ænement
    total_time = time.time() - start_time
    formatted_total_time = time.strftime("%H:%M:%S", time.gmtime(total_time))

    # R√©sum√© global apr√®s la fin de l'entra√Ænement
    st.success("Entra√Ænement termin√© ! üéâ")
    st.write(f"R√©compense moyenne : {average_reward:.2f}")
    st.write(f"Nombre moyen de pas : {average_steps:.2f}")
    st.write(f"Temps total d'entra√Ænement : {formatted_total_time}")



# Training and testing
# test_agent(agent, maze, num_episodes=100)
train_agent(agent, maze, num_episodes=100)
# test_agent(agent, maze, num_episodes=100)